#!/usr/bin/env node

/**
 * Test streaming analyzer with large dataset
 */

const { analyzeFileStreaming } = require('./dist/analyzers/streaming/streaming-analyzer');

// Memory monitoring helper
function getMemoryUsageMB() {
  const usage = process.memoryUsage();
  return {
    heapUsed: Math.round(usage.heapUsed / (1024 * 1024)),
    heapTotal: Math.round(usage.heapTotal / (1024 * 1024)),
    rss: Math.round(usage.rss / (1024 * 1024)),
    external: Math.round(usage.external / (1024 * 1024))
  };
}

// Memory tracking
const memorySnapshots = [];
let peakMemory = 0;

async function testLargeDataset(filename) {
  console.log('ğŸš€ Testing Streaming Analysis with Large Dataset');
  console.log(`ğŸ“„ File: ${filename}`);
  console.log('â”'.repeat(60));
  
  // Initial memory state
  if (global.gc) {
    global.gc(); // Force GC if available (run with --expose-gc)
  }
  
  const initialMemory = getMemoryUsageMB();
  console.log('ğŸ’¾ Initial Memory:');
  console.log(`   â€¢ Heap Used: ${initialMemory.heapUsed} MB`);
  console.log(`   â€¢ RSS: ${initialMemory.rss} MB`);
  
  const startTime = Date.now();
  
  try {
    // Configure for large dataset
    const config = {
      chunkSize: 1000,          // Process 1000 rows at a time
      memoryThresholdMB: 200,   // Stay under 200MB
      maxRowsAnalyzed: 2000000, // Allow up to 2M rows
      adaptiveChunkSizing: true
    };
    
    // Monitor memory during analysis
    const memoryInterval = setInterval(() => {
      const memory = getMemoryUsageMB();
      memorySnapshots.push({
        time: Date.now() - startTime,
        heapUsed: memory.heapUsed
      });
      peakMemory = Math.max(peakMemory, memory.heapUsed);
    }, 100); // Check every 100ms
    
    // Progress tracking
    let lastProgress = 0;
    const progressCallback = (progress) => {
      if (progress.percentage - lastProgress >= 10 || progress.percentage === 100) {
        const memory = getMemoryUsageMB();
        console.log(`\nğŸ“Š ${progress.stage.toUpperCase()} - ${progress.percentage}%`);
        console.log(`   â€¢ ${progress.message}`);
        console.log(`   â€¢ Memory: ${memory.heapUsed} MB (heap) / ${memory.rss} MB (RSS)`);
        console.log(`   â€¢ Time: ${((Date.now() - startTime) / 1000).toFixed(1)}s`);
        lastProgress = progress.percentage;
      }
    };
    
    console.log('\nğŸ”„ Starting streaming analysis...\n');
    
    const result = await analyzeFileStreaming(filename, { 
      ...config,
      progressCallback 
    });
    
    clearInterval(memoryInterval);
    
    const endTime = Date.now();
    const finalMemory = getMemoryUsageMB();
    const analysisTime = (endTime - startTime) / 1000;
    
    console.log('\n' + 'â”'.repeat(60));
    console.log('âœ… Analysis Complete!\n');
    
    console.log('ğŸ“ˆ Performance Metrics:');
    console.log(`   â€¢ Total time: ${analysisTime.toFixed(1)} seconds`);
    console.log(`   â€¢ Rows/second: ${Math.round(result.performanceMetrics.rowsAnalyzed / analysisTime).toLocaleString()}`);
    console.log(`   â€¢ Rows analyzed: ${result.performanceMetrics.rowsAnalyzed?.toLocaleString()}`);
    console.log(`   â€¢ Chunks processed: ${result.performanceMetrics.chunksProcessed}`);
    console.log(`   â€¢ Average chunk size: ${result.performanceMetrics.avgChunkSize}`);
    
    console.log('\nğŸ’¾ Memory Usage:');
    console.log(`   â€¢ Initial: ${initialMemory.heapUsed} MB`);
    console.log(`   â€¢ Peak: ${peakMemory} MB`);
    console.log(`   â€¢ Final: ${finalMemory.heapUsed} MB`);
    console.log(`   â€¢ Memory efficiency: ${result.performanceMetrics.memoryEfficiency}`);
    console.log(`   â€¢ Memory increase: ${finalMemory.heapUsed - initialMemory.heapUsed} MB`);
    
    console.log('\nğŸ“Š Dataset Info:');
    console.log(`   â€¢ Columns analyzed: ${result.metadata?.columnsAnalyzed}`);
    console.log(`   â€¢ Dataset size: ${result.metadata?.datasetSize?.toLocaleString()} rows`);
    
    if (result.warnings.length > 0) {
      console.log('\nâš ï¸  Warnings:');
      result.warnings.forEach((warning, i) => {
        if (i < 5) { // Show first 5 warnings
          console.log(`   ${i + 1}. ${warning.message}`);
        }
      });
      if (result.warnings.length > 5) {
        console.log(`   ... and ${result.warnings.length - 5} more warnings`);
      }
    }
    
    // Show sample insights
    if (result.edaAnalysis.crossVariableInsights.topFindings.length > 0) {
      console.log('\nğŸ” Sample Insights:');
      result.edaAnalysis.crossVariableInsights.topFindings.slice(0, 3).forEach(finding => {
        console.log(`   â€¢ ${finding}`);
      });
    }
    
    // Show column analysis sample
    if (result.edaAnalysis.univariateAnalysis.length > 0) {
      console.log('\nğŸ“Š Sample Column Analysis:');
      const sampleColumn = result.edaAnalysis.univariateAnalysis[0];
      console.log(`   â€¢ Column: ${sampleColumn.columnName}`);
      console.log(`   â€¢ Type: ${sampleColumn.detectedDataType}`);
      console.log(`   â€¢ Missing: ${sampleColumn.missingPercentage}%`);
      console.log(`   â€¢ Unique: ${sampleColumn.uniqueValues} values`);
    }
    
    console.log('\n' + 'â”'.repeat(60));
    console.log('ğŸ‰ SUCCESS: Large dataset processed with streaming!');
    console.log(`ğŸ’¡ Processed ${result.performanceMetrics.rowsAnalyzed?.toLocaleString()} rows`);
    console.log(`   using only ${peakMemory} MB peak memory`);
    console.log('â”'.repeat(60));
    
  } catch (error) {
    console.error('\nâŒ Analysis failed:', error.message);
    console.error('Stack:', error.stack);
    process.exit(1);
  }
}

// Get filename from command line or use default
const filename = process.argv[2] || 'test-datasets/large/transactions-large.csv';

// Run the test
console.log('ğŸš€ DataPilot Streaming Analysis Test');
console.log('â”'.repeat(60));

testLargeDataset(filename).catch(console.error);