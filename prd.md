DataPilot: Product Requirements Document (PRD) Outline

1. Introduction
* Product Name: DataPilot (TypeScript Edition)
* Vision: A lightweight, powerful CLI statistical computation engine. DataPilot does the math, so AI (or users) can derive the meaning. It serves as a "calculator on steroids" for data professionals working with CSV datasets.
* Mission: To automate the generation of comprehensive, fact-based data analysis reports, covering dataset overview, data quality, exploratory data analysis, visualization intelligence, data engineering insights, and predictive modeling guidance. The output is designed to be both human-readable and optimized for consumption by Large Language Models (LLMs).

2. Goals & Objectives
* Provide rapid, automated, and thorough analysis of CSV files.
* Generate a detailed, structured text report encompassing Sections 1-6 as designed.
* Focus on accurate statistical computations, data profiling, and well-defined heuristic-based recommendations.
* Maintain a lightweight footprint, ensuring ease of installation and use as a CLI tool.
* Empower users by providing the foundational "mathematical facts," enabling them or AI to perform higher-level interpretation and decision-making.
* Ensure the output is easily parsable and useful as context for LLMs to generate further insights.

3. Target Audience
* Primary: Data Analysts, Data Scientists, ML Engineers who need to quickly understand and prepare data.
* Secondary: Data Engineers (for schema and ETL insights), BI Professionals (for reporting foundations), technically-minded Business Users.
* Use Cases: Initial data exploration, data quality assessment, feature engineering ideation, preparing structured input for AI-driven interpretation, sanity checking datasets.

4. Scope & Core Features (Output Sections 1-6)

The primary output of DataPilot will be a single, comprehensive report generated by the datapilot all <your-data.csv> command.

* **Section 1: Comprehensive Dataset & Analysis Overview**
    * **Core Functionality:** File system metadata retrieval, robust CSV parsing (auto-detecting encoding, delimiter, headers, line endings, quoting), calculation of structural dimensions (rows, columns, cells), column name listing, estimation of in-memory size, and capturing analysis environment details.
    * **Feasibility Notes:** Highly feasible. Parsing common CSVs and gathering metadata are standard.
    * **Lightweight Focus:** Efficient parsing, well-defined heuristics for detection.

* **Section 2: Comprehensive Data Quality & Integrity Audit Report**
    * **Core Functionality:** Calculate overall and per-column completeness; assess data type conformity; evaluate uniqueness (exact duplicates, cardinality); check for format, representational, and simple logical consistency; assess value validity against detected ranges/patterns; provide heuristic quality scores and technical debt estimates.
    * **Feasibility Notes:** Most checks are feasible via direct computation, pattern matching (regex), and rule-based logic.
    * **Lightweight Focus:** Focus on deterministic checks. Advanced semantic duplicate detection or complex, domain-specific validation rules are out of scope for direct computation (though the framework could allow for user-defined rules if simple enough). DataPilot provides *indicators* for accuracy.

* **Section 3: Exploratory Data Analysis (EDA) Deep Dive**
    * **Core Functionality (Direct Computation):**
        * **Univariate:** Comprehensive descriptive statistics (mean, median, mode, stddev, variance, quantiles, IQR, etc.), frequency distributions, basic outlier detection (IQR, Z-score methods).
        * **Bivariate:** Correlation matrices (Pearson, Spearman - with significance), contingency tables, Chi-squared tests, comparison of numerical stats across categories.
        * **Distribution Shape:** Skewness, kurtosis.
    * **Core Functionality (Heuristic/Suggestive - "Maths for AI interpretation"):**
        * **Normality Tests:** Report skewness/kurtosis. For tests like Shapiro-Wilk or Jarque-Bera, if a truly lightweight TS library isn't viable, describe the distribution based on moments and *suggest* formal testing if critical for the user's context.
        * **Advanced EDA (PCA, Clustering, Deep Text/Time Series):** DataPilot will primarily *prepare data* for these analyses and *highlight their potential applicability* based on data characteristics (e.g., "High dimensionality detected, PCA may be useful for feature reduction"; "Text field 'reviews' has significant content, consider topic modeling"). It will not implement these complex algorithms natively from scratch.
    * **Feasibility Notes:** Core descriptive/bivariate stats are feasible. The approach for advanced techniques keeps it light.
    * **Lightweight Focus:** Prioritize broad univariate/bivariate coverage. For complex algorithms, set the stage for users or AI to take the next step with specialized tools.

* **Section 4: Visualization Intelligence & Chart Recommendations**
    * **Core Functionality:** Profile data for visualization; heuristically detect visual tasks (comparison, distribution, etc.); recommend standard chart types (bar, line, scatter, histogram, pie) and relevant advanced statistical graphics (violin, heatmap, etc.); specify variables, purpose, and encoding principles; provide *summarized data* needed to generate these charts; offer design tips, color palette suggestions, and A11y considerations.
    * **Feasibility Notes:** Highly feasible as it's rule-based, content-driven, and requires data aggregation (already part of EDA).
    * **Lightweight Focus:** **No rendering of visualizations.** This is critical. DataPilot provides the "recipe" and "ingredients" for charts, not the kitchen.

* **Section 5: Data Engineering & Structural Insights**
    * **Core Functionality:** Analyze current schema; recommend an optimized target schema (SQL DDL format); suggest data type conversions; heuristically identify primary/foreign key candidates and potential relationships; recommend transformations for missing data, outliers, encoding, scaling, and feature engineering (for analytics & ML); assess ML readiness; provide high-level scalability/storage considerations; output a conceptual "persistent knowledge base" snippet.
    * **Feasibility Notes:** Highly feasible. Schema generation, heuristic inferences, and transformation *recommendations* are based on prior analysis.
    * **Lightweight Focus:** Recommends engineering steps and provides structural DDL; does not perform complex ETL operations itself.

* **Section 6: Predictive Modeling & Advanced Analytics Guidance**
    * **Core Functionality:** Identify potential modeling tasks (regression, classification, clustering); suggest appropriate algorithms (with a focus on Linear/Logistic Regression, Decision Trees/CART [your interest], and common ensembles); explain relevant methodologies (e.g., **CART pruning, Regression Residuals Analysis** [your interest]); guide on model building workflow (feature selection, splitting, tuning, C-V, imbalance handling); advise on model evaluation and interpretation; highlight ethical AI considerations.
    * **Feasibility Notes:** Highly feasible as it involves providing structured expert guidance and explanations, not executing ML training.
    * **Lightweight Focus:** Purely a guidance and knowledge-sharing module.

5. Technical Requirements & Design Philosophy
* Language/Platform: TypeScript, Node.js (latest LTS version).
* Architecture: CLI application. Modular design for different analysis engines (Quality, EDA, VisRec, EngRec, ModelGuide).
* Lightweight Definition:
* Minimal external dependencies (prefer small, focused, well-maintained libraries if any; e.g., for CSV parsing, basic stats).
* Target fast execution times for moderately sized CSVs (e.g., <1GB, <1-5 million rows) on typical developer hardware.
* Keep installed footprint small.
* Core Philosophy:
* Maths First: Prioritize accurate computation of statistical facts.
* AI for Interpretation: Structure output for optimal use by LLMs or human experts for subjective interpretation.
* Simplicity & Ease of Use: Minimal configuration, sensible defaults, clear CLI. Auto-detect encodings, delimiters, etc., where feasible.
* Not Over-Engineered: Avoid premature optimization or implementing overly complex algorithms natively if it compromises the lightweight goal. Stick to providing the "facts" and "strong suggestions."
* Output: Primarily a single, comprehensive, structured text report (Markdown compatible). Optional JSON/YAML for knowledge base snippets.
* Cross-Platform: Must run on Windows, macOS, and Linux.
* Offline Capability: Must function without internet access.

6. Feasibility & Prioritization of Advanced Features
* In Scope for Direct Computation (V1): All descriptive statistics, frequency counts, correlations (Pearson, Spearman), Chi-squared, IQR/Z-score outliers, data quality checks (as defined), schema generation, transformation recommendations, visualization recommendations and summarized data prep.
* Heuristic / Suggestive (V1): Normality assessment (based on moments, with pointers to formal tests), PK/FK inference, visual task detection, modeling task identification, algorithm suggestions.
* Out of Scope for Direct Computation (V1 - to maintain lightweight nature):
* Full implementation of PCA, advanced clustering algorithms (K-Means might be borderline if a light library exists, but hierarchical/DBSCAN are out).
* Advanced NLP (topic modeling, full NER from scratch).
* Advanced Time Series modeling (ARIMA, Prophet from scratch).
* Rationale: Implementing these robustly and efficiently in pure TS without heavy libraries would make the tool non-lightweight and significantly increase development complexity. DataPilot will instead prepare data for and suggest the use of these techniques with specialized tools where appropriate.

7. Non-Goals
* Will not render or display charts/visualizations directly.
* Will not perform automated machine learning model training, tuning, or deployment.
* Will not provide natural language narrative interpretations or business summaries (this is the role of the user or an external AI consuming DataPilot's output).
* Will not be a GUI application (CLI only for V1).
* Will not connect to live databases or APIs for data input (CSV files only for V1).

8. Success Metrics
* Accuracy: Correctness of all statistical calculations and factual data reported.
* Completeness: All defined subsections of the report (1-6) are generated with relevant information for a given dataset.
* Performance: Execution time and memory usage within defined targets for benchmark datasets.
* Usability: Positive user feedback on ease of use, clarity of CLI, and comprehensibility of the report.
* Utility for AI: Demonstrable effectiveness of the output as high-quality context for LLMs to perform interpretation and further analysis.
* Lightweight Target: Meeting defined goals for installation size and dependency footprint.